{
  "name": "wassam-localai-ext",
  "displayName": "Wassam Local AI Extension",
  "description": "AI Code Helper that runs locally on your computer with the help of Ollama.",
  "version": "1.1.1",
  "publisher": "wassam",
  "repository": {
    "type": "git",
    "url": "https://github.com/wassamz/wassam-localai-ext.git"
  },
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/wassamz/wassam-localai-ext/issues"
  },
  "homepage": "https://github.com/wassamz/wassam-localai-ext#readme",
  "engines": {
    "vscode": "^1.96.0"
  },
  "icon": "images/wassam-icon-256.png",
  "categories": [
    "AI"
  ],
  "activationEvents": [],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "wassamLocalAI.chat",
        "title": "Local AI Chat"
      }
    ],
    "configuration": {
      "title": "Wassam Local AI Model",
      "properties": {
        "wassam-localai-ext.localModel": {
          "type": "string",
          "default": "deepseek-r1:1.5b",
          "description": "Configure the local LLM model to use that you've installed with Ollama. Default is deepseek-r1:1.5b."
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "pretest": "npm run compile && npm run lint",
    "lint": "eslint src",
    "test": "vscode-test"
  },
  "devDependencies": {
    "@types/mocha": "^10.0.10",
    "@types/node": "20.x",
    "@types/vscode": "^1.96.0",
    "@typescript-eslint/eslint-plugin": "^8.22.0",
    "@typescript-eslint/parser": "^8.22.0",
    "@vscode/test-cli": "^0.0.10",
    "@vscode/test-electron": "^2.4.1",
    "eslint": "^9.19.0",
    "typescript": "^5.7.3"
  },
  "dependencies": {
    "ollama": "^0.5.12"
  }
}
